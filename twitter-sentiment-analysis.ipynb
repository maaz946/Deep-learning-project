{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* **Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and many more\n",
    "\n",
    "* **Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n",
    "\n",
    "In this notebook, we'll develop a **Sentiment Analysis model** to categorize a tweet as **Positive or Negative.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "1. [Importing dependencies](#p1)\n",
    "2. [Importing dataset](#p2)\n",
    "3. [Exploratory Data Analysis](#p2-a)\n",
    "4. [Preprocessing Text](#p3)\n",
    "5. [Analysing data](#p4)\n",
    "6. [Splitting data](#p5)\n",
    "7. [TF-IDF Vectoriser](#p6)\n",
    "8. [Transforming Dataset](#p7)\n",
    "9. [Creating and Evaluating Models](#p8)\n",
    "    * [BernoulliNB Model](#p8-1)\n",
    "    * [LinearSVC Model](#p8-2)\n",
    "    * [Logistic Regression Model](#p8-3)\n",
    "    * [Cat Boost Model](#p8-4)\n",
    "    * [LightGBM Model](#p8-5)\n",
    "    * [Gradient Boosting Model](#p8-6)\n",
    "    * [Naive Bayes Model](#p8-7)\n",
    "10. [Saving the Models](#p9)\n",
    "11. [Using the Model](#p10)\n",
    "12. [Model Testing](#p11)\n",
    "\n",
    "## <a name=\"p1\">Importing Dependencies</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:14:56.782244Z",
     "iopub.status.busy": "2022-05-30T17:14:56.781768Z",
     "iopub.status.idle": "2022-05-30T17:15:01.081064Z",
     "shell.execute_reply": "2022-05-30T17:15:01.079467Z",
     "shell.execute_reply.started": "2022-05-30T17:14:56.78216Z"
    }
   },
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit (nltk)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Model Buildig (sklearn)\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "#Data Splitting and Traininig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p2\">Importing dataset</a>\n",
    "The dataset being used is the **sentiment140 dataset**. It contains 1,600,000 tweets extracted using the **Twitter API**. The tweets have been annotated **(0 = Negative, 4 = Positive)** and they can be used to detect sentiment.\n",
    " \n",
    "*[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]*\n",
    "\n",
    "**It contains the following 6 fields:**\n",
    "1. **sentiment**: the polarity of the tweet *(0 = negative, 4 = positive)*\n",
    "2. **ids**: The id of the tweet *(2087)*\n",
    "3. **date**: the date of the tweet *(Sat May 16 23:58:44 UTC 2009)*\n",
    "4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. **user**: the user that tweeted *(robotickilldozr)*\n",
    "6. **text**: the text of the tweet *(Lyx is cool)*\n",
    "\n",
    "We require only the **sentiment** and **text** fields, so we discard the rest.\n",
    "\n",
    "Furthermore, we're changing the **sentiment** field so that it has new values to reflect the sentiment. **(0 = Negative, 1 = Positive)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:01.084492Z",
     "iopub.status.busy": "2022-05-30T17:15:01.083774Z",
     "iopub.status.idle": "2022-05-30T17:15:01.101611Z",
     "shell.execute_reply": "2022-05-30T17:15:01.100299Z",
     "shell.execute_reply.started": "2022-05-30T17:15:01.084355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting the data from kaggle\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:01.104993Z",
     "iopub.status.busy": "2022-05-30T17:15:01.104013Z",
     "iopub.status.idle": "2022-05-30T17:15:08.403186Z",
     "shell.execute_reply": "2022-05-30T17:15:08.402114Z",
     "shell.execute_reply.started": "2022-05-30T17:15:01.104952Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding_used = \"ISO-8859-1\"\n",
    "tweets_df = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = encoding_used)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:08.405385Z",
     "iopub.status.busy": "2022-05-30T17:15:08.40504Z",
     "iopub.status.idle": "2022-05-30T17:15:13.746783Z",
     "shell.execute_reply": "2022-05-30T17:15:13.745717Z",
     "shell.execute_reply.started": "2022-05-30T17:15:08.405354Z"
    }
   },
   "outputs": [],
   "source": [
    "data_columns  = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "encoding_used = \"ISO-8859-1\"\n",
    "\n",
    "tweets_df = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = encoding_used, names = data_columns )\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"#p2-a\">Exploratory Data Analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:13.748982Z",
     "iopub.status.busy": "2022-05-30T17:15:13.748273Z",
     "iopub.status.idle": "2022-05-30T17:15:14.816171Z",
     "shell.execute_reply": "2022-05-30T17:15:14.815284Z",
     "shell.execute_reply.started": "2022-05-30T17:15:13.748934Z"
    }
   },
   "outputs": [],
   "source": [
    "target_group = tweets_df.groupby('target').count()['text']\n",
    "target_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:14.818464Z",
     "iopub.status.busy": "2022-05-30T17:15:14.817314Z",
     "iopub.status.idle": "2022-05-30T17:15:15.02669Z",
     "shell.execute_reply": "2022-05-30T17:15:15.025774Z",
     "shell.execute_reply.started": "2022-05-30T17:15:14.818421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the distribution for dataset.\n",
    "ax = target_group.plot(kind='bar', title='Distribution of data', legend=False)\n",
    "ax.set_xticklabels(['Negative','Positive'], rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)**\n",
    "* **This means we only have negative and positive labels.**\n",
    "* **I will change all positive labels to 1 i.e all 4 == 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a > Keeping only relevant values </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.028199Z",
     "iopub.status.busy": "2022-05-30T17:15:15.027897Z",
     "iopub.status.idle": "2022-05-30T17:15:15.035694Z",
     "shell.execute_reply": "2022-05-30T17:15:15.03477Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.028173Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.037534Z",
     "iopub.status.busy": "2022-05-30T17:15:15.037084Z",
     "iopub.status.idle": "2022-05-30T17:15:15.08725Z",
     "shell.execute_reply": "2022-05-30T17:15:15.086205Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.037491Z"
    }
   },
   "outputs": [],
   "source": [
    "data = tweets_df[['target', 'text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.088561Z",
     "iopub.status.busy": "2022-05-30T17:15:15.088249Z",
     "iopub.status.idle": "2022-05-30T17:15:15.09893Z",
     "shell.execute_reply": "2022-05-30T17:15:15.097987Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.088525Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns = [\"sentiment\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.103543Z",
     "iopub.status.busy": "2022-05-30T17:15:15.102868Z",
     "iopub.status.idle": "2022-05-30T17:15:15.156742Z",
     "shell.execute_reply": "2022-05-30T17:15:15.15553Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.103494Z"
    }
   },
   "outputs": [],
   "source": [
    "data[data['sentiment'] != 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.158856Z",
     "iopub.status.busy": "2022-05-30T17:15:15.158286Z",
     "iopub.status.idle": "2022-05-30T17:15:15.187093Z",
     "shell.execute_reply": "2022-05-30T17:15:15.185856Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.158817Z"
    }
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.189288Z",
     "iopub.status.busy": "2022-05-30T17:15:15.18858Z",
     "iopub.status.idle": "2022-05-30T17:15:15.2339Z",
     "shell.execute_reply": "2022-05-30T17:15:15.23281Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.189244Z"
    }
   },
   "outputs": [],
   "source": [
    "#Confirming 4 == 1\n",
    "data[data['sentiment'] != 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.235649Z",
     "iopub.status.busy": "2022-05-30T17:15:15.235204Z",
     "iopub.status.idle": "2022-05-30T17:15:15.430313Z",
     "shell.execute_reply": "2022-05-30T17:15:15.429257Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.235602Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to a list data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.432571Z",
     "iopub.status.busy": "2022-05-30T17:15:15.432193Z",
     "iopub.status.idle": "2022-05-30T17:15:15.889191Z",
     "shell.execute_reply": "2022-05-30T17:15:15.888105Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.432529Z"
    }
   },
   "outputs": [],
   "source": [
    "text, sentiment = list(data['text']), list(data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Output of Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.891585Z",
     "iopub.status.busy": "2022-05-30T17:15:15.891105Z",
     "iopub.status.idle": "2022-05-30T17:15:15.907446Z",
     "shell.execute_reply": "2022-05-30T17:15:15.906529Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.891542Z"
    }
   },
   "outputs": [],
   "source": [
    "text[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.909015Z",
     "iopub.status.busy": "2022-05-30T17:15:15.908675Z",
     "iopub.status.idle": "2022-05-30T17:15:15.922499Z",
     "shell.execute_reply": "2022-05-30T17:15:15.921393Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.908984Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment[0:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p3\">Preprocess Text</a>\n",
    "**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n",
    "\n",
    "**The Preprocessing steps taken are:**\n",
    "1. **Lower Casing:** Each text is converted to lowercase. #Helps to keep things normalized\n",
    "2. **Replacing URLs:** Links starting with **\"http\" or \"https\" or \"www\"** are replaced by **\"URL\"**.\n",
    "3. **Replacing Emojis:** Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. *(eg: \":)\" to \"EMOJIsmile\")*\n",
    "4. **Replacing Usernames:** Replace @Usernames with word **\"USER\"**. *(eg: \"@Kaggle\" to \"USER\")*\n",
    "5. **Removing Non-Alphabets:** Replacing characters except Digits and Alphabets with a space.\n",
    "6. **Removing Consecutive letters:** 3 or more consecutive letters are replaced by 2 letters. *(eg: \"Heyyyy\" to \"Heyy\")*\n",
    "7. **Removing Short Words:** Words with length less than 2 are removed.\n",
    "8. **Removing Stopwords:** Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. *(eg: \"the\", \"he\", \"have\")*\n",
    "9. **Lemmatizing:** Lemmatization is the process of converting a word to its base form. *(e.g: “Great” to “Good”)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Emojis and their meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.924599Z",
     "iopub.status.busy": "2022-05-30T17:15:15.92412Z",
     "iopub.status.idle": "2022-05-30T17:15:15.984282Z",
     "shell.execute_reply": "2022-05-30T17:15:15.983419Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.924553Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:15.986062Z",
     "iopub.status.busy": "2022-05-30T17:15:15.985342Z",
     "iopub.status.idle": "2022-05-30T17:15:15.99964Z",
     "shell.execute_reply": "2022-05-30T17:15:15.99866Z",
     "shell.execute_reply.started": "2022-05-30T17:15:15.986029Z"
    }
   },
   "outputs": [],
   "source": [
    "mystopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:16.001338Z",
     "iopub.status.busy": "2022-05-30T17:15:16.001002Z",
     "iopub.status.idle": "2022-05-30T17:15:16.026067Z",
     "shell.execute_reply": "2022-05-30T17:15:16.025212Z",
     "shell.execute_reply.started": "2022-05-30T17:15:16.001307Z"
    }
   },
   "outputs": [],
   "source": [
    "english_stop_words =  stopwords.words('english')\n",
    "english_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:16.028101Z",
     "iopub.status.busy": "2022-05-30T17:15:16.027391Z",
     "iopub.status.idle": "2022-05-30T17:15:16.037513Z",
     "shell.execute_reply": "2022-05-30T17:15:16.03635Z",
     "shell.execute_reply.started": "2022-05-30T17:15:16.028061Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwordlist = stopwords.words('english') + mystopwordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:16.039717Z",
     "iopub.status.busy": "2022-05-30T17:15:16.039103Z",
     "iopub.status.idle": "2022-05-30T17:15:16.053418Z",
     "shell.execute_reply": "2022-05-30T17:15:16.052356Z",
     "shell.execute_reply.started": "2022-05-30T17:15:16.039681Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    #creating a Lemmatizer\n",
    "    wordLemma = WordNetLemmatizer() #define the imported library\n",
    "    \n",
    "    # Defining regular expression pattern we can find. in tweets\n",
    "    \n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" # e.g check out https://dot.com for more\n",
    "    userPattern       = '@[^\\s]+' # e.g @FagbamigbeK check this out\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\" # e.g I am *10 better!\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"  # e.g Heyyyyyyy, I am back!\n",
    "    seqReplacePattern = r\"\\1\\1\" # e.g Replace Heyyyyyyy with Heyy\n",
    "    \n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower() #normalizing all text to a lower case\n",
    "        \n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet) #using the substitution method of the regular expression library\n",
    "        \n",
    "        \n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys(): #in each of the looped tweet, replace each emojis with their respective meaning\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])  # emojis[emoji] helps to get the value of the emoji from the dictionary\n",
    "            \n",
    "            \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)  #To hide Personal Information, we can replace all usernames with User\n",
    "        \n",
    "        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet) # e.g I am *10 better!\n",
    "        \n",
    "        \n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet) # e.g Replace Heyyyyyyy with Heyy\n",
    "        \n",
    "        \n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            if len(word) > 2 and word.isalpha():\n",
    "                word = wordLemma.lemmatize(word)\n",
    "                tweetwords += (word + ' ')\n",
    "        \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noting the time text preprocessing took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:15:16.055252Z",
     "iopub.status.busy": "2022-05-30T17:15:16.054761Z",
     "iopub.status.idle": "2022-05-30T17:18:12.095599Z",
     "shell.execute_reply": "2022-05-30T17:18:12.093968Z",
     "shell.execute_reply.started": "2022-05-30T17:15:16.055207Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "preprocessedtext = preprocess(text) #the preprocess function at work\n",
    "print(f'Text Processing Done.')\n",
    "print(f'Time taken for text processing: {round(time.time()-t)} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T06:58:41.564618Z",
     "iopub.status.busy": "2022-05-30T06:58:41.563859Z",
     "iopub.status.idle": "2022-05-30T06:58:41.570302Z",
     "shell.execute_reply": "2022-05-30T06:58:41.569154Z",
     "shell.execute_reply.started": "2022-05-30T06:58:41.56458Z"
    }
   },
   "source": [
    "## <a name=\"p4\">Analysing the data</a>\n",
    "Now we're going to analyse the preprocessed data to get an understanding of it. We'll plot **Word Clouds** for **Positive and Negative** tweets from our dataset and see which words occur the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:12.098154Z",
     "iopub.status.busy": "2022-05-30T17:18:12.097775Z",
     "iopub.status.idle": "2022-05-30T17:18:12.106642Z",
     "shell.execute_reply": "2022-05-30T17:18:12.104893Z",
     "shell.execute_reply.started": "2022-05-30T17:18:12.098122Z"
    }
   },
   "outputs": [],
   "source": [
    "text[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:12.108805Z",
     "iopub.status.busy": "2022-05-30T17:18:12.108383Z",
     "iopub.status.idle": "2022-05-30T17:18:12.127181Z",
     "shell.execute_reply": "2022-05-30T17:18:12.126232Z",
     "shell.execute_reply.started": "2022-05-30T17:18:12.108769Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessedtext[0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:12.128936Z",
     "iopub.status.busy": "2022-05-30T17:18:12.128578Z",
     "iopub.status.idle": "2022-05-30T17:18:12.180849Z",
     "shell.execute_reply": "2022-05-30T17:18:12.179767Z",
     "shell.execute_reply.started": "2022-05-30T17:18:12.128903Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_sentiments = preprocessedtext[:800000]\n",
    "negative_sentiments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:12.18293Z",
     "iopub.status.busy": "2022-05-30T17:18:12.182427Z",
     "iopub.status.idle": "2022-05-30T17:18:13.105995Z",
     "shell.execute_reply": "2022-05-30T17:18:13.104358Z",
     "shell.execute_reply.started": "2022-05-30T17:18:12.182887Z"
    }
   },
   "outputs": [],
   "source": [
    "data_neg = []\n",
    "for words in negative_sentiments:\n",
    "    words = words.lower().replace(\"user\",\"\")\n",
    "    words = words.lower().replace(\"url\",\"\")\n",
    "    data_neg.append(words)\n",
    "    \n",
    "data_neg[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:13.108115Z",
     "iopub.status.busy": "2022-05-30T17:18:13.10769Z",
     "iopub.status.idle": "2022-05-30T17:18:13.115512Z",
     "shell.execute_reply": "2022-05-30T17:18:13.113746Z",
     "shell.execute_reply.started": "2022-05-30T17:18:13.108081Z"
    }
   },
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(max_words = 1000 , width = 1600 , height = 800, collocations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:13.121156Z",
     "iopub.status.busy": "2022-05-30T17:18:13.120298Z",
     "iopub.status.idle": "2022-05-30T17:18:38.49518Z",
     "shell.execute_reply": "2022-05-30T17:18:38.493855Z",
     "shell.execute_reply.started": "2022-05-30T17:18:13.121077Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "negative_wc = word_cloud.generate(\" \".join(data_neg))\n",
    "plt.imshow(negative_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Cloud for Positive tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:38.496709Z",
     "iopub.status.busy": "2022-05-30T17:18:38.496338Z",
     "iopub.status.idle": "2022-05-30T17:18:38.515106Z",
     "shell.execute_reply": "2022-05-30T17:18:38.513833Z",
     "shell.execute_reply.started": "2022-05-30T17:18:38.496678Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_sentiments = preprocessedtext[800000:]\n",
    "positive_sentiments[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:38.517006Z",
     "iopub.status.busy": "2022-05-30T17:18:38.516599Z",
     "iopub.status.idle": "2022-05-30T17:18:39.38054Z",
     "shell.execute_reply": "2022-05-30T17:18:39.379375Z",
     "shell.execute_reply.started": "2022-05-30T17:18:38.51697Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pos = []\n",
    "for words in positive_sentiments:\n",
    "    words = words.lower().replace(\"user\",\"\")\n",
    "    words = words.lower().replace(\"url\",\"\")\n",
    "    data_pos.append(words)\n",
    "    \n",
    "data_pos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:18:39.383168Z",
     "iopub.status.busy": "2022-05-30T17:18:39.382243Z",
     "iopub.status.idle": "2022-05-30T17:19:03.765943Z",
     "shell.execute_reply": "2022-05-30T17:19:03.764804Z",
     "shell.execute_reply.started": "2022-05-30T17:18:39.383106Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "positive_wc = word_cloud.generate(\" \".join(data_pos))\n",
    "plt.imshow(positive_wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p7\">Tranforming the dataset</a>\n",
    "Transforming the **X_train** and **X_test** dataset into matrix of **TF-IDF Features** by using the **TF-IDF Vectoriser**. This datasets will be used to train the model and test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:19:03.767676Z",
     "iopub.status.busy": "2022-05-30T17:19:03.767295Z",
     "iopub.status.idle": "2022-05-30T17:19:05.037439Z",
     "shell.execute_reply": "2022-05-30T17:19:05.036456Z",
     "shell.execute_reply.started": "2022-05-30T17:19:03.767641Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessedtext, sentiment,\n",
    "                                                    test_size = 0.05, random_state = 0)\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p6\">TF-IDF Vectoriser</a>\n",
    "##### Term Frequency Inverse Document Frequency\n",
    "**TF-IDF indicates what the importance of the word is in order to understand the document or dataset.** Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it’s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n",
    "\n",
    "**TF-IDF Vectoriser** converts a collection of raw documents to a **matrix of TF-IDF features**. The **Vectoriser** is usually trained on only the **X_train** dataset. \n",
    "\n",
    "**ngram_range**  is the range of number of words in a sequence. *[e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]*\n",
    "\n",
    "**max_features** specifies the number of features to consider. *[Ordered by feature frequency across the corpus]*.\n",
    "\n",
    "- gives weight to each word and tells how important the word is. \n",
    "- Importances increases proportionally to the number of times a word appears in the sentence but is penalized by the frequency of the word in all the sentences\n",
    "- Weight is the product of term frquency(frequency of a word occuring in a sentence) and inverse document frequency(measures how important the word is)\n",
    "- weight = term frequency * inverse document frequency\n",
    "- ranage(1,2) means vectorizer will consider a single word or pair of word for calculation\n",
    "- range(2,2) means a pair of word only\n",
    "- strip_accent to protect against unwanted encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:19:05.039751Z",
     "iopub.status.busy": "2022-05-30T17:19:05.039284Z",
     "iopub.status.idle": "2022-05-30T17:20:28.09486Z",
     "shell.execute_reply": "2022-05-30T17:20:28.093021Z",
     "shell.execute_reply.started": "2022-05-30T17:19:05.039696Z"
    }
   },
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2),stop_words = stopwordlist, strip_accents = 'unicode', max_features = 500000)\n",
    "vectoriser.fit(X_train) #fit the training data\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p7\">Tranforming the dataset</a>\n",
    "Transforming the **X_train** and **X_test** dataset into matrix of **TF-IDF Features** by using the **TF-IDF Vectoriser**. This datasets will be used to train the model and test against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:20:28.097491Z",
     "iopub.status.busy": "2022-05-30T17:20:28.096931Z",
     "iopub.status.idle": "2022-05-30T17:21:23.318384Z",
     "shell.execute_reply": "2022-05-30T17:21:23.316948Z",
     "shell.execute_reply.started": "2022-05-30T17:20:28.097428Z"
    }
   },
   "outputs": [],
   "source": [
    "#transform the training and test data\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p8\">Creating and Evaluating Models</a>\n",
    "\n",
    "We're creating 3 different types of model for our sentiment analysis problem: \n",
    "* **Bernoulli Naive Bayes (BernoulliNB)**\n",
    "* **Linear Support Vector Classification (LinearSVC)**\n",
    "* **Logistic Regression (LR)**\n",
    "* **Category and Boosting (Cat Boost)**\n",
    "* **Light Gradient Boosting Machine(LightGBM)**\n",
    "\n",
    "Since our dataset is not **skewed**, i.e. it has equal number of **Positive and Negative** Predictions. We're choosing **Accuracy** as our evaluation metric. Furthermore, we're plotting the **Confusion Matrix** to get an understanding of how our model is performing on both classification types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:21:23.322098Z",
     "iopub.status.busy": "2022-05-30T17:21:23.320078Z",
     "iopub.status.idle": "2022-05-30T17:21:23.328438Z",
     "shell.execute_reply": "2022-05-30T17:21:23.326847Z",
     "shell.execute_reply.started": "2022-05-30T17:21:23.322044Z"
    }
   },
   "outputs": [],
   "source": [
    "# To use other algoritm we have to convert the categorical variables to numerical variables with one-hot encoding, ordinal encoding, label encoding,dummy variable e.t.c but the text contained here might make that blow up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:21:23.330581Z",
     "iopub.status.busy": "2022-05-30T17:21:23.330115Z",
     "iopub.status.idle": "2022-05-30T17:21:23.348246Z",
     "shell.execute_reply": "2022-05-30T17:21:23.346896Z",
     "shell.execute_reply.started": "2022-05-30T17:21:23.330547Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test) #Xtest is not used in model training\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos'] #configuration of a confusin matrix\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)] #converting confusion matrix value to percentage in 2 decimal places.\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-1\">BernoulliNB Model</a>\n",
    "\n",
    "    * [BernoulliNB Model](#p8-1)\n",
    "    * [LinearSVC Model](#p8-2)\n",
    "    * [Logistic Regression Model](#p8-3)\n",
    "    * [Gradient Boosting Model](#p8-4)\n",
    "    * [Naive Bayes Model](#p8-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:21:23.351Z",
     "iopub.status.busy": "2022-05-30T17:21:23.35018Z",
     "iopub.status.idle": "2022-05-30T17:21:25.01633Z",
     "shell.execute_reply": "2022-05-30T17:21:25.01526Z",
     "shell.execute_reply.started": "2022-05-30T17:21:23.350952Z"
    }
   },
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-2\">LinearSVC Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:21:25.017959Z",
     "iopub.status.busy": "2022-05-30T17:21:25.017613Z",
     "iopub.status.idle": "2022-05-30T17:22:14.580307Z",
     "shell.execute_reply": "2022-05-30T17:22:14.579165Z",
     "shell.execute_reply.started": "2022-05-30T17:21:25.01793Z"
    }
   },
   "outputs": [],
   "source": [
    "SVCmodel = LinearSVC()\n",
    "SVCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(SVCmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-3\">Logistic Regression Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:22:14.581981Z",
     "iopub.status.busy": "2022-05-30T17:22:14.581589Z",
     "iopub.status.idle": "2022-05-30T17:26:57.268705Z",
     "shell.execute_reply": "2022-05-30T17:26:57.267751Z",
     "shell.execute_reply.started": "2022-05-30T17:22:14.581949Z"
    }
   },
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "model_Evaluate(LRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-4\">Cat Boost Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:26:57.271059Z",
     "iopub.status.busy": "2022-05-30T17:26:57.270021Z",
     "iopub.status.idle": "2022-05-30T17:26:57.276513Z",
     "shell.execute_reply": "2022-05-30T17:26:57.274741Z",
     "shell.execute_reply.started": "2022-05-30T17:26:57.271015Z"
    }
   },
   "outputs": [],
   "source": [
    "# #categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "# CatBoostmodel = CatBoostClassifier(cat_features = preprocessedtext, eval_metric = (X_test, y_test))\n",
    "# CatBoostmodel.fit(X_train, y_train)\n",
    "# model_Evaluate(CatBoostmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-5\">LightGBM Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T20:40:18.947334Z",
     "iopub.status.busy": "2022-05-30T20:40:18.946378Z",
     "iopub.status.idle": "2022-05-30T20:40:18.950326Z",
     "shell.execute_reply": "2022-05-30T20:40:18.94989Z",
     "shell.execute_reply.started": "2022-05-30T20:40:18.94729Z"
    }
   },
   "outputs": [],
   "source": [
    "# LGBMmodel = LGBMClassifier()\n",
    "# LGBMmodel.fit(X_train, y_train)\n",
    "# model_Evaluate(LGBMmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-6\">Gradient Boosting Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:29:33.938877Z",
     "iopub.status.busy": "2022-05-30T17:29:33.937077Z",
     "iopub.status.idle": "2022-05-30T17:50:04.373011Z",
     "shell.execute_reply": "2022-05-30T17:50:04.371541Z",
     "shell.execute_reply.started": "2022-05-30T17:29:33.938818Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GRDBCmodel = GradientBoostingClassifier()\n",
    "GRDBCmodel.fit(X_train, y_train)\n",
    "model_Evaluate(GRDBCmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"p8-7\">Naive Bayes Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:50:04.550704Z",
     "iopub.status.busy": "2022-05-30T17:50:04.549989Z",
     "iopub.status.idle": "2022-05-30T17:50:05.860381Z",
     "shell.execute_reply": "2022-05-30T17:50:05.858548Z",
     "shell.execute_reply.started": "2022-05-30T17:50:04.550671Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NBClassifier_model = MultinomialNB()\n",
    "NBClassifier_model.fit(X_train, y_train)\n",
    "model_Evaluate(NBClassifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the **Logistic Regression Model** performs the best out of all the different models that we tried. It achieves nearly **80% accuracy** while classifying the sentiment of a tweet.\n",
    "\n",
    "Although it should also be noted that the **BernoulliNB Model** is the fastest to train and predict on. It also achieves **78% accuracy** while calssifying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p9\">Saving the Models</a>\n",
    "We're using **PICKLE** to save **Vectoriser, BernoulliNB, Logistic Regression Model, Linear Support Vector Classification and Light Gradient Boosting Machine** for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:26:57.312175Z",
     "iopub.status.busy": "2022-05-30T17:26:57.311594Z",
     "iopub.status.idle": "2022-05-30T17:27:02.677231Z",
     "shell.execute_reply": "2022-05-30T17:27:02.676047Z",
     "shell.execute_reply.started": "2022-05-30T17:26:57.312141Z"
    }
   },
   "outputs": [],
   "source": [
    "#Vectoriser\n",
    "file = open('vectoriser-ngram-(1,2).pickle','wb')\n",
    "pickle.dump(vectoriser, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "#Bernoulli\n",
    "file = open('Sentiment-BNB.pickle','wb')\n",
    "pickle.dump(BNBmodel, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "#Linear Regression\n",
    "file = open('Sentiment-LR.pickle','wb')\n",
    "pickle.dump(LRmodel, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "#SVCmodel\n",
    "file = open('Sentiment-SVCmodel.pickle','wb')\n",
    "pickle.dump(SVCmodel, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "#CatBoost\n",
    "# file = open('Sentiment-CatBoost.pickle','wb')\n",
    "# pickle.dump(CatBoostmodel, file)\n",
    "# file.close()\n",
    "\n",
    "\n",
    "#LightGBM\n",
    "# file = open('Sentiment-LightGBM.pickle','wb')\n",
    "# pickle.dump(LightGBM, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p10\">Using the Model.</a>\n",
    "\n",
    "To use the model for **Sentiment Prediction** we need to import the **Vectoriser** and **LR Model** using **Pickle**.\n",
    "\n",
    "The vectoriser can be used to transform data to matrix of TF-IDF Features.\n",
    "While the model can be used to predict the sentiment of the transformed Data.\n",
    "The text whose sentiment has to be predicted however must be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:27:02.679392Z",
     "iopub.status.busy": "2022-05-30T17:27:02.679036Z",
     "iopub.status.idle": "2022-05-30T17:27:02.689314Z",
     "shell.execute_reply": "2022-05-30T17:27:02.687991Z",
     "shell.execute_reply.started": "2022-05-30T17:27:02.679361Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Replace '..path/' by the path of the saved models.\n",
    "    '''\n",
    "    \n",
    "    # Load the vectoriser.\n",
    "    file = open('..path/vectoriser-ngram-(1,2).pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "    # Load the LR Model.\n",
    "    file = open('..path/Sentiment-LRv1.pickle', 'rb')\n",
    "    LRmodel = pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectoriser, LRmodel\n",
    "\n",
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectoriser.transform(preprocess(text)) #Passing the tweet through the processing stage and transforming it with the vectoriser\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"]) #Replacing the class of 0 and 1 with Negative and Positive respectively\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"p11\">Model Testing.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-30T17:27:02.69141Z",
     "iopub.status.busy": "2022-05-30T17:27:02.690899Z",
     "iopub.status.idle": "2022-05-30T17:27:02.80703Z",
     "shell.execute_reply": "2022-05-30T17:27:02.805973Z",
     "shell.execute_reply.started": "2022-05-30T17:27:02.691365Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Loading the models.\n",
    "    #vectoriser, LRmodel = load_models()\n",
    "    \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"I Love Google!\",\n",
    "            \"May the Good Lord be with you.\", \"I hate peanuts!\",\n",
    "            \"Mr. Kehinde, what are you doing next? this is great!\"]\n",
    "    \n",
    "    df = predict(vectoriser, LRmodel, text)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
